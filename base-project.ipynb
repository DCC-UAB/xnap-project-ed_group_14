{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm # progress bar\n",
    "from sklearn.model_selection import train_test_split # Dividing train test\n",
    "from nltk.translate.bleu_score import corpus_bleu # BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf \n",
    "\n",
    "# tf.enable_eager_execution(tf.ConfigProto(log_device_placement=True)) \n",
    "# habilita l'execució ansiosa(eager execution) en Tensorflow i mostra el registre d'ubicacio de dispositius.\n",
    "# l'execicio ansiosa es ina forma d'execucio de gràfics que permet una interaccio mes interactiva amb el model i un fluxe de treball mes similar a la programacio interactiva.\n",
    "# Els calculs es realitzen inmediatament i els resultat es retornen de inmediat\n",
    "# el argument aquest tf.ConfigProto(log_device_placement=True) es opcional i s'utilitza per imprimir un registre que indica en quin dispositiu s'executa cada operacio en Tensorflow --> optimitza rendiment i compren millor com sulitza la capacitat de processament del hardware\n",
    "\n",
    "# print(tf.add([1.0, 2.0], [3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data\" # es el path\n",
    "dataset_images_path = dataset_path + \"/Images/\"  # path de imatges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 180 # altura\n",
    "img_width = 180 # amplada\n",
    "validation_split = 0.2 # 80% entrenament 20% validacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last layer of the Inception V3 model\n",
    "def get_encoder():\n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet') \n",
    "    # crea una instancia de l'arquitectura de red neuronal convolucional preentrenada en el conjunt de dades imagenet\n",
    "    new_input = image_model.input\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "    # creacio del model amb el nou input i hidden\n",
    "    return image_features_extract_model\n",
    "\n",
    "# Preprocess the caption, splitting the string and adding <start> and <end> tokens\n",
    "def get_preprocessed_caption(caption):    \n",
    "    caption = re.sub(r'\\s+', ' ', caption) #reemplaza totes les sequencies de un o mes espais en blanc per un sol espai\n",
    "    caption = caption.strip() #elimina els caracters en blanc\n",
    "    caption = \"<start> \" + caption + \" <end>\" # agrega el start i el end al caption\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_captions_dict = {} # creem diccionari\n",
    "\n",
    "with open(dataset_path + \"/captions.txt\", \"r\") as dataset_info: #obrim dataset\n",
    "    next(dataset_info) # Omit header: image, caption\n",
    "\n",
    "    # Using a subset of 4,000 entries out of 40,000\n",
    "    for info_raw in list(dataset_info)[:4000]: # itera a través dels primers 4000 element del dataset\n",
    "        info = info_raw.split(\",\") # divideix la cadena en una llista de subcadenes utilitzant la coma com delimitador\n",
    "        image_filename = info[0] # el nom del fitxer esta a la posicio 1\n",
    "        caption = get_preprocessed_caption(info[1]) # processem el que hi ha en la posicio 2 de la subcadena\n",
    "\n",
    "        if image_filename not in images_captions_dict.keys(): # si el nom del fitxer no esta dintre del diccionari\n",
    "            images_captions_dict[image_filename] = [caption] # l'afegim amb el value processat\n",
    "        else: # si ja esta en el diccionari\n",
    "            images_captions_dict[image_filename].append(caption) # afegim el value processat a la key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregar una imatge\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(dataset_images_path + image_path)#llegir el fitxer que es troba en aquell path\n",
    "    img = tf.image.decode_jpeg(img, channels=3) #decodificat una imatge JPEG codificada en una cadena binaria i retorna una representacio de tensor de la imatge decodificada\n",
    "    img = tf.image.resize(img, (img_height, img_width)) # redimensionem la imatge amb la altura i amplada que volem\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img) # preprocessing needed for pre-trained model\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_captions_dict_keys = list(images_captions_dict.keys()) # les imatges que tenim\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(image_captions_dict_keys) #crea un objecte de conjunt de TensorFlow a partir de un Tensor d'entrada.\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64) #operacio de preprocessament d'un conjunt de dades que carrega i decodifica imatges en paralel i les agrrupa en subatchts de 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dict = {} #diccionari buit\n",
    "encoder = get_encoder() # model\n",
    "for img_tensor, path_tensor in tqdm(image_dataset): #es un bucle d'un conjunt de dades de TensorFlow que conte tensors de imatge i rutes d'arxiu, el tqdm() es una fucnio per proporcionar una barra de progress en temps real mentres s'itera.\n",
    "    batch_features_tensor = encoder(img_tensor) #retorna la imatge codificada\n",
    "    \n",
    "    # Loop over batch to save each element in images_dict\n",
    "    for batch_features, path in zip(batch_features_tensor, path_tensor):\n",
    "        decoded_path = path.numpy().decode(\"utf-8\") #tensor de cadena de bytes --> cadena de caracters de Python\n",
    "        images_dict[decoded_path] = batch_features.numpy() #tensorflow --> array python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(images_dict.items())[0][1].shape # retorna la forma del tensor de la imatge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(load_image('1000268201_693b08cb0e.jpg')[0].numpy()) #mostra una imatge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenir les labels\n",
    "def get_images_labels(image_filenames):\n",
    "    images = [] #llista de imatges\n",
    "    labels = [] #llista de labels\n",
    "    \n",
    "    for image_filename in image_filenames: # per cada imatge\n",
    "        image = images_dict[image_filename] #agafar el seu value\n",
    "        captions = images_captions_dict[image_filename] #agafar la seva caption\n",
    "\n",
    "        # Add one instance per caption\n",
    "        for caption in captions: #si hi ha mes de una caption afegirles totes\n",
    "            images.append(image)\n",
    "            labels.append(caption)\n",
    "            \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = list(images_captions_dict.keys()) # totes les imatges que hi ha a images_caption_dict\n",
    "image_filenames_train, image_filenames_test = \\\n",
    "    train_test_split(image_filenames, test_size=validation_split, random_state=1) # fer el split entre train i test\n",
    "\n",
    "X_train, y_train_raw = get_images_labels(image_filenames_train) #agafar les labels de train\n",
    "X_test, y_test_raw = get_images_labels(image_filenames_test)#agafar les labels de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per image 5 captions and 0.2 test split\n",
    "len(X_train), len(y_train_raw), len(X_test), len(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_k = 5000 # Take maximum of words out of 7600\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ') # s'utilitza per vectoritzar text i convertirlo en sequencia de numeros\n",
    "\n",
    "# Generate vocabulary from train captions\n",
    "tokenizer.fit_on_texts(y_train_raw)\n",
    "\n",
    "# Introduce padding to make the captions of the same size for the LSTM model\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "y_train = tokenizer.texts_to_sequences(y_train_raw)\n",
    "\n",
    "# Add padding to each vector to the max_length of the captions (automatically done)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_caption_length = max(len(t) for t in y_train) #agafar la longitud maxima de les etiquetes de entrenament\n",
    "print(max_caption_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.index_word[i] for i in y_train[1]] # convertir una etiqueta de una llista en una llista de paraules utilitzant un objecte tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "#crear un objecte tf.data.Dataset a partir de X i Y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train) #especificar el tamany del bufer\n",
    "BATCH_SIZE = 64 #especificar el tamany del batch\n",
    "NUM_STEPS = BUFFER_SIZE // BATCH_SIZE #numero de pasos d'entrenament\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Using prefetching: https://www.tensorflow.org/guide/data_performance#prefetching\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.flat = tf.keras.layers.Flatten() #aplanar una entrada de dades multidimensionals\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim) #, activation='relu') #una capa completament connectada, que s'utilitza per transformar una entrada de una dimensio a una altre\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        # input_dim = size of the vocabulary\n",
    "        # Define the embedding layer to transform the input caption sequence\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Define the Long Short Term Memory layer to predict the next words in the sequence \n",
    "        self.lstm = tf.keras.layers.LSTM(self.units, return_sequences=True, return_state=True)\n",
    "            \n",
    "        # Define a dense layer to transform the LSTM output into prediction of the best word\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size) #, activation='softmax')\n",
    "\n",
    "    # A function that transforms the input embeddings and passes them to the LSTM layer \n",
    "    def call(self, captions, features, omit_features = False, initial_state = None, verbose = False):\n",
    "        if verbose:\n",
    "            print(\"Before embedding\")\n",
    "            print(captions.shape)\n",
    "\n",
    "        embed = self.embedding(captions) #(batch_size, 1, embedding_dim)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Embed\")\n",
    "            print(embed.shape)\n",
    "\n",
    "        features = tf.expand_dims(features, 1) #expandir les dimensions del tensor, 1 mes.\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Features\")\n",
    "            print(features.shape)\n",
    "        \n",
    "        # Concatenating the image and caption embeddings before providing them to LSTM\n",
    "        # shape == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        lstm_input = tf.concat([features, embed], axis=-2) if (omit_features == False) else embed\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"LSTM input\")\n",
    "            print(lstm_input.shape)\n",
    "\n",
    "        # Passing the concatenated vector to the LSTM\n",
    "        output, memory_state, carry_state = self.lstm(lstm_input, initial_state=initial_state)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"LSTM output\")\n",
    "            print(output.shape)\n",
    "\n",
    "        # Transform LSTM output units to vocab_size\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, memory_state, carry_state\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units)) # crea un tensor de forma (batch_size, self.units), inicialitzant tot  a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = embedding_dim = 512 # As in the paper\n",
    "vocab_size = min(top_k + 1, len(tokenizer.word_index.keys())) #limitar el numero de paraules que s'utilitzen per entrenar el model\n",
    "\n",
    "# Initialize encoder and decoder\n",
    "encoder = CNN_Encoder(embedding_dim) #capa de red neuronal convolucional utilitzada per extreure caracteristiques d'una imatge\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size) #es una classe que defineix una red neuronal recurrent utilitzada per generear subtitul a partir de les cracteristiques de imatges extretes per el encoder\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# As the label is not one-hot encoded but indices. Logits as they are not probabilities.\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "# Computes the loss using SCCE and calculates the average of singular losses in the tensor\n",
    "def loss_function(real, pred, verbose=False):\n",
    "    loss_ = loss_object(real, pred) #calcula la perduda entra la real i la predita\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loss\")\n",
    "        print(loss_)\n",
    "    \n",
    "    loss_ = tf.reduce_mean(loss_, axis = 1) #calcula la mitja entre els elements al llarg de un eix especific de un tensor.\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"After Mean Axis 1\")    \n",
    "        print(loss_)\n",
    "\n",
    "    return loss_\n",
    "# Key Point: Any Python side-effects (appending to a list, printing with print, etc) will only happen once, when func is traced. \n",
    "# To have side-effects executed into your tf.function they need to be written as TF ops:\n",
    "@tf.function\n",
    "def train_step(img_tensor, target, verbose=False):    \n",
    "    if verbose:\n",
    "        print(\"Image tensor\")\n",
    "        print(img_tensor.shape)\n",
    "\n",
    "        print(\"Target\")\n",
    "        print(target.shape)    \n",
    "\n",
    "    # The input would be each set of words without the last one (<end>), to leave space for the first one that\n",
    "    # would be the image embedding\n",
    "    dec_input = tf.convert_to_tensor(target[:, :-1])\n",
    "\n",
    "    # Source: https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "    with tf.GradientTape() as tape: #tf.GradientTape() calcul automatic de gradients\n",
    "        features = encoder(img_tensor)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Features CNN\")\n",
    "            print(features)\n",
    "            \n",
    "        predictions, _, _ = decoder(dec_input, features, verbose=verbose) #instancia de RNN_Decoder, realitza una inferencia cap endevant       \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Predictions RNN\")\n",
    "            print(predictions)\n",
    "        \n",
    "        caption_loss = loss_function(target, predictions) # (batch_size, )\n",
    "\n",
    "        # After tape\n",
    "        total_batch_loss = tf.reduce_sum(caption_loss) # Sum (batch_size, ) => K\n",
    "        mean_batch_loss = tf.reduce_mean(caption_loss) # Mean(batch_size, ) => K\n",
    "\n",
    "    # Updated the variables\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(caption_loss, trainable_variables) #calcular els gradient de les variables entrenables, a la perduda utilitzant la diferenciacio automatica\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables)) # aplica els gradients calculats a les variables entrenables del model\n",
    "\n",
    "    return total_batch_loss, mean_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\" #path de chechpoint\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer) #guardar i carregar els pesos del model\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5) #administra punts de control de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0 #la primera epoca es el 0\n",
    "if ckpt_manager.latest_checkpoint: #agafar el ultim checkpoint\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1]) #agafa el numero del ultim punt de control guardat.\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "EPOCHS = 20\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    real_epoch = len(loss_plot) + 1\n",
    "    start = time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        total_batch_loss, mean_batch_loss = train_step(img_tensor, target, verbose=False) #es una funcio que executa un pas d'entrenament en un batch de imatges\n",
    "        total_loss += total_batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Batch Loss {:.4f}'.format(real_epoch, batch, mean_batch_loss.numpy()))\n",
    "    \n",
    "    print ('Total Loss {:.6f}'.format(total_loss))\n",
    "    epoch_loss = total_loss / NUM_STEPS\n",
    "    \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(epoch_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Epoch Loss {:.6f}'.format(real_epoch, epoch_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot) #ensenya la loss\n",
    "plt.xlabel('Epochs') #label de x\n",
    "plt.ylabel('Loss') #label de y\n",
    "plt.title('Loss Plot') #label de title\n",
    "plt.show() #mostrarho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(caption):\n",
    "    return [item for item in caption if item not in ['<start>', '<end>', '<pad>']]\n",
    "\n",
    "#aquesta funcio rep una llista de paraules que representa una oracio de una imatge i elimina el tokends start,end,pad, \n",
    "#que son els que utilitza el model per indica inici i fi de la oracio i la llargada.\n",
    "# de forma que retorna una oracio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_name = random.choice(image_filenames_train) # selecciona aleatoriament un elemnt de una llista. En aquest cas agafar una imatge al atzar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get captions from a test image\n",
    "def get_caption(img):    \n",
    "    # Add image to an array to simulate batch size of 1    \n",
    "    features = encoder(tf.expand_dims(img, 0))\n",
    "    \n",
    "    caption = []\n",
    "    dec_input = tf.expand_dims([], 0) #crear un tensor 1D buit i l'exte a 2D\n",
    "    \n",
    "     # Inputs the image embedding into the trained LSTM layer and predicts the first word of the sequence.\n",
    "    # The output, hidden and cell states are passed again to the LSTM to generate the next word.\n",
    "    # The iteration is repeated until the caption does not reach the max length.\n",
    "    state = None\n",
    "    for i in range(1, max_caption_length):\n",
    "        predictions, memory_state, carry_state = \\\n",
    "            decoder(dec_input, features, omit_features=i > 1, initial_state=state) #s'executa el decder per generar la predicio següent i els estat de memoria i celda.\n",
    "\n",
    "        # Takes maximum index of predictions\n",
    "        word_index = np.argmax(predictions.numpy().flatten())\n",
    "\n",
    "        caption.append(tokenizer.index_word[word_index])\n",
    "\n",
    "        dec_input = tf.expand_dims([word_index], 0)  #afegeix una dimensio mes a word_index     \n",
    "        state = [memory_state, carry_state]\n",
    "    \n",
    "    # Filter caption\n",
    "    return clean_caption(caption)\n",
    "\n",
    "raw_img = load_image(test_img_name)[0] #es una funcio que retorna una imatge en forma de tensor\n",
    "img = images_dict[test_img_name] #value de test_img_name\n",
    "captions = images_captions_dict[test_img_name] #value de test_img_name\n",
    "\n",
    "plt.imshow(raw_img) #mostra la imatge\n",
    "\n",
    "print(\"Real captions\")\n",
    "for caption in captions: #mostra totes les captions d'aquella imatge\n",
    "    print(caption)\n",
    "\n",
    "print(\"Esimated caption\")\n",
    "estimated_caption = get_caption(img) #et va mostrant les oracions de cada imatge\n",
    "print(estimated_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "directml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
