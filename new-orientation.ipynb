{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import gc\n",
    "import random\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/captions.txt\", sep=',')\n",
    "print(len(df))\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_char_word(word_list):\n",
    "    lst = []\n",
    "    for word in word_list:\n",
    "        if len(word)>1:\n",
    "            lst.append(word)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_caption'] = df['caption'].apply(lambda caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in caption.split(\" \")] + ['<end>'])\n",
    "df['cleaned_caption']  = df['cleaned_caption'].apply(lambda x : remove_single_char_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq_len'] = df['cleaned_caption'].apply(lambda x : len(x))\n",
    "max_seq_len = df['seq_len'].max()\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['seq_len'], axis = 1, inplace = True)\n",
    "df['cleaned_caption'] = df['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = df['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\n",
    "word_dict = Counter(word_list)\n",
    "word_dict =  sorted(word_dict, key=word_dict.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_dict))\n",
    "print(word_dict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_dict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index: word for index, word in enumerate(word_dict)}\n",
    "word_to_index = {word: index for index, word in enumerate(word_dict)}\n",
    "print(len(index_to_word), len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_seq']  = df['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = 'image')\n",
    "train = df.iloc[:int(0.9*len(df))]\n",
    "valid = df.iloc[int(0.9*len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train), train['image'].nunique())\n",
    "print(len(valid), valid['image'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(train)\n",
    "print(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_train_imgs = train[['image']].drop_duplicates()\n",
    "unq_valid_imgs = valid[['image']].drop_duplicates()\n",
    "print(len(unq_train_imgs), len(unq_valid_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractImageFeatureResNetDataSet():\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "        self.scaler = transforms.Resize([224, 224])\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    def __len__(self):  \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_name = self.data.iloc[idx]['image']\n",
    "        img_loc = 'data/Images/'+str(image_name)\n",
    "\n",
    "        img = Image.open(img_loc)\n",
    "        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n",
    "\n",
    "        return image_name, t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\n",
    "train_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\n",
    "valid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "resnet18.eval()\n",
    "list(resnet18._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resNet18Layer4 = resnet18._modules.get('layer4').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(t_img):\n",
    "    \n",
    "    t_img = Variable(t_img)\n",
    "    my_embedding = torch.zeros(1, 512, 7, 7)\n",
    "    def copy_data(m, i, o):\n",
    "        my_embedding.copy_(o.data)\n",
    "    \n",
    "    h = resNet18Layer4.register_forward_hook(copy_data)\n",
    "    resnet18(t_img)\n",
    "    \n",
    "    h.remove()\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_imgFtr_ResNet_train = {}\n",
    "for image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n",
    "    t_img = t_img.to(device)\n",
    "    embdg = get_vector(t_img)\n",
    "    \n",
    "    extract_imgFtr_ResNet_train[image_name[0]] = embdg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./EncodedImageTrainResNet.pkl\", \"wb\")\n",
    "pickle.dump(extract_imgFtr_ResNet_train, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_imgFtr_ResNet_valid = {}\n",
    "for image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n",
    "    t_img = t_img.to(device)\n",
    "    embdg = get_vector(t_img)\n",
    " \n",
    "    extract_imgFtr_ResNet_valid[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./EncodedImageValidResNet.pkl\", \"wb\")\n",
    "pickle.dump(extract_imgFtr_ResNet_valid, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataSetResnet():\n",
    "    def __init__(self, data, pkl_file):\n",
    "        self.data = data\n",
    "        self.encodedImgs = pd.read_pickle(pkl_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        caption_seq = self.data.iloc[idx]['text_seq']\n",
    "        target_seq = caption_seq[1:]+[0]\n",
    "\n",
    "        image_name = self.data.iloc[idx]['image']\n",
    "        image_tensor = self.encodedImgs[image_name]\n",
    "        image_tensor = image_tensor.permute(0,2,3,1)\n",
    "        image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n",
    "\n",
    "        return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\n",
    "train_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\n",
    "valid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pe.size(0) < x.size(0):\n",
    "            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n",
    "        self.pe = self.pe[:x.size(0), : , : ]\n",
    "        \n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n",
    "        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n",
    "        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_size)\n",
    "        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.last_linear_layer.bias.data.zero_()\n",
    "        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n",
    "        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n",
    "        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_size)\n",
    "        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.last_linear_layer.bias.data.zero_()\n",
    "        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def generate_Mask(self, size, decoder_inp):\n",
    "        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n",
    "\n",
    "        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n",
    "        decoder_input_pad_mask_bool = decoder_inp == 0\n",
    "\n",
    "        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n",
    "\n",
    "    def forward(self, encoded_image, decoder_inp):\n",
    "        encoded_image = encoded_image.permute(1,0,2)\n",
    "        \n",
    "\n",
    "        decoder_inp_embed = self.embedding(decoder_inp)* math.sqrt(self.embedding_size)\n",
    "        \n",
    "        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n",
    "        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n",
    "        \n",
    "\n",
    "        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(decoder_inp.size(1), decoder_inp)\n",
    "        decoder_input_mask = decoder_input_mask.to(device)\n",
    "        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n",
    "        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n",
    "        \n",
    "\n",
    "        decoder_output = self.TransformerDecoder(tgt = decoder_inp_embed, memory = encoded_image, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n",
    "        \n",
    "        final_output = self.last_linear_layer(decoder_output)\n",
    "\n",
    "        return final_output,  decoder_input_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ictModel = ImageCaptionModel(16, 4, vocab_size, 512).to(device)\n",
    "optimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "min_val_loss = np.float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(EPOCH)):\n",
    "    total_epoch_train_loss = 0\n",
    "    total_epoch_valid_loss = 0\n",
    "    total_train_words = 0\n",
    "    total_valid_words = 0\n",
    "    ictModel.train()\n",
    "\n",
    "    ### Train Loop\n",
    "    for caption_seq, target_seq, image_embed in train_dataloader_resnet:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_embed = image_embed.squeeze(1).to(device)\n",
    "        caption_seq = caption_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n",
    "        output = output.permute(1, 2, 0)\n",
    "\n",
    "        loss = criterion(output,target_seq)\n",
    "\n",
    "        loss_masked = torch.mul(loss, padding_mask)\n",
    "\n",
    "        final_batch_loss = torch.sum(loss_masked)/torch.sum(padding_mask)\n",
    "\n",
    "        final_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n",
    "        total_train_words += torch.sum(padding_mask)\n",
    "\n",
    " \n",
    "    total_epoch_train_loss = total_epoch_train_loss/total_train_words\n",
    "  \n",
    "\n",
    "    ### Eval Loop\n",
    "    ictModel.eval()\n",
    "    with torch.no_grad():\n",
    "        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n",
    "\n",
    "            image_embed = image_embed.squeeze(1).to(device)\n",
    "            caption_seq = caption_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n",
    "            output = output.permute(1, 2, 0)\n",
    "\n",
    "            loss = criterion(output,target_seq)\n",
    "\n",
    "            loss_masked = torch.mul(loss, padding_mask)\n",
    "\n",
    "            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n",
    "            total_valid_words += torch.sum(padding_mask)\n",
    "\n",
    "    total_epoch_valid_loss = total_epoch_valid_loss/total_valid_words\n",
    "  \n",
    "    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n",
    "  \n",
    "    if min_val_loss > total_epoch_valid_loss:\n",
    "        print(\"Writing Model at epoch \", epoch)\n",
    "        torch.save(ictModel, './BestModel')\n",
    "        min_val_loss = total_epoch_valid_loss\n",
    "  \n",
    "\n",
    "    scheduler.step(total_epoch_valid_loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./BestModel')\n",
    "start_token = word_to_index['<start>']\n",
    "end_token = word_to_index['<end>']\n",
    "pad_token = word_to_index['<pad>']\n",
    "max_seq_len = 33\n",
    "print(start_token, end_token, pad_token)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
