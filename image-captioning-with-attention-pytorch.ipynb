{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1) Initial Imports and loading the utils function. The dataset is used is <a href='https://www.kaggle.com/adityajn105/flickr8k'>Flickr 8k</a> from kaggle.<br>Custom dataset and dataloader is implemented in <a href=\"https://www.kaggle.com/mdteach/torch-data-loader-flicker-8k\">this</a> notebook."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"f48ec521-cf45-4fcc-b266-16de6ca56488","_uuid":"b60eed4a-4184-49f4-b1fb-af2c50754b1e","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/raul/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["#location of the training data \n","data_location =  \"data\"\n","\n","#imports\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader,Dataset\n","import torchvision.transforms as T\n","import pandas as pd\n","\n","#imports \n","import os\n","from collections import Counter\n","# import spacy\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader,Dataset\n","import torchvision.transforms as T\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","from PIL import Image"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch_directml\n","device = torch_directml.device()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","class Vocabulary:\n","    def __init__(self,freq_threshold):\n","        #setting the pre-reserved tokens int to string tokens\n","        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n","        \n","        #string to int tokens\n","        #its reverse dict self.itos\n","        self.stoi = {v:k for k,v in self.itos.items()}\n","        \n","        self.freq_threshold = freq_threshold\n","\n","        \n","    def __len__(self): return len(self.itos)\n","    \n","    @staticmethod\n","    def tokenize(text):\n","        return word_tokenize(text.lower())\n","    \n","    def build_vocab(self, sentence_list):\n","        frequencies = Counter()\n","        idx = 4\n","        \n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                frequencies[word] += 1\n","                \n","                #add the word to the vocab if it reaches minum frequecy threshold\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","    \n","    def numericalize(self,text):\n","        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n","        tokenized_text = self.tokenize(text)\n","        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]    "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class FlickrDataset(Dataset):\n","    \"\"\"\n","    FlickrDataset\n","    \"\"\"\n","    def __init__(self,root_dir,caption_file,transform=None,freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(caption_file)\n","        self.transform = transform\n","        \n","        #Get image and caption colum from the dataframe\n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","        \n","        #Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocab(self.captions.tolist())\n","        \n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        caption = self.captions[idx]\n","        img_name = self.imgs[idx]\n","        img_location = os.path.join(self.root_dir,img_name)\n","        img = Image.open(img_location).convert(\"RGB\")\n","        \n","        #apply the transfromation to the image\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        \n","        #numericalize the caption text\n","        caption_vec = []\n","        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n","        caption_vec += self.vocab.numericalize(caption)\n","        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n","        \n","        return img, torch.tensor(caption_vec)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class CapsCollate:\n","    \"\"\"\n","    Collate to apply the padding to the captions with dataloader\n","    \"\"\"\n","    def __init__(self,pad_idx,batch_first=False):\n","        self.pad_idx = pad_idx\n","        self.batch_first = batch_first\n","    \n","    def __call__(self,batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs,dim=0)\n","        \n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n","        return imgs,targets"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_data_loader(dataset,batch_size,shuffle=False,num_workers=1):\n","    \"\"\"\n","    Returns torch dataloader for the flicker8k dataset\n","    \n","    Parameters\n","    -----------\n","    dataset: FlickrDataset\n","        custom torchdataset named FlickrDataset \n","    batch_size: int\n","        number of data to load in a particular batch\n","    shuffle: boolean,optional;\n","        should shuffle the datasests (default is False)\n","    num_workers: int,optional\n","        numbers of workers to run (default is 1)  \n","    \"\"\"\n","\n","    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","    collate_fn = CapsCollate(pad_idx=pad_idx,batch_first=True)\n","\n","    data_loader = DataLoader(\n","        dataset=dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        collate_fn=collate_fn\n","    )\n","\n","    return data_loader"]},{"attachments":{},"cell_type":"markdown","metadata":{"trusted":true},"source":["### 2) **<b>Implementing the Helper function to plot the Tensor image**"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["#show the tensor image\n","import matplotlib.pyplot as plt\n","def show_image(img, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    \n","    #unnormalize \n","    img[0] = img[0] * 0.229\n","    img[1] = img[1] * 0.224 \n","    img[2] = img[2] * 0.225 \n","    img[0] += 0.485 \n","    img[1] += 0.456 \n","    img[2] += 0.406\n","    \n","    img = img.numpy().transpose((1, 2, 0))\n","    \n","    \n","    plt.imshow(img)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["device(type='privateuseone', index=0)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#Initiate the Dataset and Dataloader\n","\n","#setting the constants\n","data_location =  \"data\" # --> data\n","BATCH_SIZE = 64\n","# BATCH_SIZE = 6\n","NUM_WORKER = 4\n","\n","#defining the transform to be applied\n","transforms = T.Compose([\n","    T.Resize(226),                     \n","    T.RandomCrop(224),                 \n","    T.ToTensor(),                               \n","    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n","])\n","\n","\n","#testing the dataset class\n","dataset =  FlickrDataset(\n","    root_dir = data_location+\"/Images\",\n","    caption_file = data_location+\"/captions.txt\",\n","    transform=transforms\n",")\n","\n","#writing the dataloader\n","data_loader = get_data_loader(\n","    dataset=dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=NUM_WORKER,\n","    shuffle=True,\n","    # batch_first=False\n",")\n","\n","\n","\n","\n","#vocab_size\n","vocab_size = len(dataset.vocab)\n","\n","#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"attachments":{},"cell_type":"markdown","metadata":{"trusted":true},"source":["### 3) Defining the Model Architecture\n","\n","Model is seq2seq model. In the **encoder** pretrained ResNet model is used to extract the features. Decoder, is the implementation of the Bahdanau Attention Decoder. In the decoder model **LSTM cell**."]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import torchvision.models as models\n","from torch.utils.data import DataLoader,Dataset\n","import torchvision.transforms as T"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super(EncoderCNN, self).__init__()\n","        resnet = models.resnet50(pretrained=True)\n","        for param in resnet.parameters():\n","            param.requires_grad_(False)\n","        \n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","        \n","\n","    def forward(self, images):\n","        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n","        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n","        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n","        return features\n"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["#Bahdanau Attention\n","class Attention(nn.Module):\n","    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n","        super(Attention, self).__init__()\n","        \n","        self.attention_dim = attention_dim\n","        \n","        self.W = nn.Linear(decoder_dim,attention_dim)\n","        self.U = nn.Linear(encoder_dim,attention_dim)\n","        \n","        self.A = nn.Linear(attention_dim,1)\n","        \n","        \n","        \n","        \n","    def forward(self, features, hidden_state):\n","        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n","        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n","        \n","        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n","        \n","        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n","        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n","        \n","        \n","        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n","        \n","        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n","        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n","        \n","        return alpha,attention_weights\n","        "]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":["#Attention Decoder\n","class DecoderRNN(nn.Module):\n","    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n","        super().__init__()\n","        \n","        #save the model param\n","        self.vocab_size = vocab_size\n","        self.attention_dim = attention_dim\n","        self.decoder_dim = decoder_dim\n","        \n","        self.embedding = nn.Embedding(vocab_size,embed_size)\n","        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n","        \n","        \n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n","        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n","        \n","        \n","        self.fcn = nn.Linear(decoder_dim,vocab_size)\n","        self.drop = nn.Dropout(drop_prob)\n","        \n","        \n","    \n","    def forward(self, features, captions):\n","        \n","        #vectorize the caption\n","        embeds = self.embedding(captions)\n","        \n","        # Initialize LSTM state\n","        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n","        \n","        #get the seq length to iterate\n","        seq_length = len(captions[0])-1 #Exclude the last one\n","        batch_size = captions.size(0)\n","        num_features = features.size(1)\n","        \n","        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n","                \n","        for s in range(seq_length):\n","            alpha,context = self.attention(features, h)\n","            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","                    \n","            output = self.fcn(self.drop(h))\n","            \n","            preds[:,s] = output\n","            alphas[:,s] = alpha  \n","        \n","        \n","        return preds, alphas\n","    \n","    def generate_caption(self,features,max_len=20,vocab=None):\n","        # Inference part\n","        # Given the image features generate the captions\n","        \n","        batch_size = features.size(0)\n","        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n","        \n","        alphas = []\n","        \n","        # Starting input\n","        word = torch.tensor([vocab.stoi['<SOS>']] * batch_size).view(batch_size, 1).to(device)\n","        embeds = self.embedding(word)\n","        \n","        captions = []\n","        \n","        for i in range(max_len):\n","            alpha, context = self.attention(features, h)\n","            \n","            # Store the alpha score\n","            alphas.append(alpha.cpu().detach().numpy())\n","            \n","            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","            output = self.fcn(self.drop(h))\n","            output = output.view(batch_size, -1)\n","            \n","            # Select the word with the highest value\n","            predicted_word_idx = output.argmax(dim=1)\n","            \n","            # Save the generated word\n","            captions.append(predicted_word_idx.tolist())\n","            print(predicted_word_idx)\n","            # Check if <EOS> detected for any sample\n","            eos_detected = [vocab.itos[pred_word_idx.item()] == \"<EOS>\" for pred_word_idx in predicted_word_idx]\n","            \n","            # End if <EOS> detected for all samples\n","            if all(eos_detected):\n","                break\n","            \n","            # Send generated words as the next caption input\n","            embeds = self.embedding(predicted_word_idx.unsqueeze(1))\n","        \n","        # Convert vocab indices to words and return sentences\n","        captions = [[vocab.itos[idx] for idx in caption] for caption in captions]\n","        \n","        return captions, alphas\n","    \n","    \n","    def init_hidden_state(self, encoder_out):\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out)\n","        return h, c\n"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n","        super().__init__()\n","        self.encoder = EncoderCNN()\n","        self.decoder = DecoderRNN(\n","            embed_size=embed_size,\n","            vocab_size = len(dataset.vocab),\n","            attention_dim=attention_dim,\n","            encoder_dim=encoder_dim,\n","            decoder_dim=decoder_dim\n","        )\n","        \n","    def forward(self, images, captions):\n","        features = self.encoder(images)\n","        outputs = self.decoder(features, captions)\n","        return outputs\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4) Setting Hypperparameter and Init the model"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["#Hyperparams\n","embed_size=300\n","vocab_size = len(dataset.vocab)\n","attention_dim=256\n","encoder_dim=2048\n","decoder_dim=512\n","learning_rate = 3e-4\n"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/raul/miniconda3/envs/directml-pytorch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/home/raul/miniconda3/envs/directml-pytorch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["#init model\n","model = EncoderDecoder(\n","    embed_size=300,\n","    vocab_size = len(dataset.vocab),\n","    attention_dim=256,\n","    encoder_dim=2048,\n","    decoder_dim=512\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[],"source":["#helper function to save the model\n","def save_model(model,num_epochs):\n","    model_state = {\n","        'num_epochs':num_epochs,\n","        'embed_size':embed_size,\n","        'vocab_size':len(dataset.vocab),\n","        'attention_dim':attention_dim,\n","        'encoder_dim':encoder_dim,\n","        'decoder_dim':decoder_dim,\n","        'state_dict':model.state_dict()\n","    }\n","\n","    torch.save(model_state,'attention_model_state.pth')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5) Training Job from above configs"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([64, 3, 224, 224])\n","torch.Size([7, 3, 224, 224])\n","epoch 1\n"]}],"source":["num_epochs = 1\n","print_every = 1000\n","\n","for epoch in range(1,num_epochs+1):   \n","    for idx, (image, captions) in enumerate(iter(data_loader)):\n","        image,captions = image.to(device),captions.to(device)\n","\n","        # Zero the gradients.\n","        optimizer.zero_grad()\n","\n","        # Feed forward\n","        print(image.shape)\n","        outputs,attentions = model(image, captions)\n","\n","        # Calculate the batch loss.\n","        targets = captions[:,1:]\n","        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n","        \n","        # Backward pass.\n","        loss.backward()\n","\n","        # Update the parameters in the optimizer.\n","        optimizer.step()\n","\n","        if (idx+1)%print_every == 0:\n","            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n","            \n","            \n","            #generate the caption\n","            model.eval()\n","            with torch.no_grad():\n","                dataiter = iter(data_loader)\n","                img,_ = next(dataiter)\n","                features = model.encoder(img[0:1].to(device))\n","                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","                caption = ' '.join(caps)\n","                show_image(img[0],title=caption)\n","                \n","            model.train()\n","        \n","    #save the latest model\n","    save_model(model,epoch)\n","    print(f'epoch {epoch}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 6 Visualizing the attentions\n","Defining helper functions\n","<li>Given the image generate captions and attention scores</li>\n","<li>Plot the attention scores in the image</li>"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"ename":"IndentationError","evalue":"expected an indented block after function definition on line 1 (2778510722.py, line 5)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[18], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    for epoch in range(1,num_epochs+1):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 1\n"]}],"source":["def test(model, ):\n","\n","\n","# Testing ...   \n","for epoch in range(1,num_epochs+1):   \n","    for idx, (image, captions) in enumerate(iter(data_loader)):\n","        image,captions = image.to(device),captions.to(device)\n","\n","        # Zero the gradients.\n","        #optimizer.zero_grad()\n","\n","        # Feed forward\n","        outputs,attentions = model(image, captions)\n","\n","        # Calculate the batch loss.\n","        targets = captions[:,1:]\n","        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n","        \n","        # Backward pass.\n","        loss.backward()\n","\n","        # Update the parameters in the optimizer.\n","        #optimizer.step()\n","\n","        if (idx+1)%print_every == 0:\n","            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n","            \n","            \n","            #generate the caption\n","            model.eval()\n","            with torch.no_grad():\n","                dataiter = iter(data_loader)\n","                img,_ = next(dataiter)\n","                features = model.encoder(img[0:1].to(device))\n","                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","                caption = ' '.join(caps)\n","                #show_image(img[0],title=caption)\n","                \n","        \n","    #save the latest model\n","    save_model(model,epoch)"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[],"source":["#generate caption\n","def get_caps_from(features_tensors):\n","    #generate the caption\n","    model.eval()\n","    with torch.no_grad():\n","        features = model.encoder(features_tensors.to(device))\n","        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","        caption = ' '.join(caps)\n","        show_image(features_tensors[0],title=caption)\n","    \n","    return caps,alphas\n","\n","#Show attention\n","def plot_attention(img, result, attention_plot):\n","    #untransform\n","    img[0] = img[0] * 0.229\n","    img[1] = img[1] * 0.224 \n","    img[2] = img[2] * 0.225 \n","    img[0] += 0.485 \n","    img[1] += 0.456 \n","    img[2] += 0.406\n","    \n","    img = img.numpy().transpose((1, 2, 0))\n","    temp_image = img\n","\n","    fig = plt.figure(figsize=(15, 15))\n","\n","    len_result = len(result)\n","    for l in range(len_result):\n","        temp_att = attention_plot[l].reshape(7,7)\n","        \n","        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n","        ax.set_title(result[l])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n","        \n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n","tensor([4], device='privateuseone:0')\n"]},{"ename":"TypeError","evalue":"sequence item 0: expected str instance, list found","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m img \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mclone()\n\u001b[1;32m      6\u001b[0m img1 \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mclone()\n\u001b[0;32m----> 7\u001b[0m caps,alphas \u001b[39m=\u001b[39m get_caps_from(img\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m      9\u001b[0m plot_attention(img1, caps, alphas)\n","Cell \u001b[0;32mIn[36], line 8\u001b[0m, in \u001b[0;36mget_caps_from\u001b[0;34m(features_tensors)\u001b[0m\n\u001b[1;32m      6\u001b[0m     features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder(features_tensors\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m      7\u001b[0m     caps,alphas \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mgenerate_caption(features,vocab\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m----> 8\u001b[0m     caption \u001b[39m=\u001b[39m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(caps)\n\u001b[1;32m      9\u001b[0m     show_image(features_tensors[\u001b[39m0\u001b[39m],title\u001b[39m=\u001b[39mcaption)\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m caps,alphas\n","\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"]}],"source":["#show any 1\n","dataiter = iter(data_loader)\n","images,_ = next(dataiter)\n","\n","img = images[0].detach().clone()\n","img1 = images[0].detach().clone()\n","caps,alphas = get_caps_from(img.unsqueeze(0))\n","\n","plot_attention(img1, caps, alphas)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#show any 1\n","dataiter = iter(data_loader)\n","images,_ = next(dataiter)\n","\n","img = images[0].detach().clone()\n","img1 = images[0].detach().clone()\n","caps,alphas = get_caps_from(img.unsqueeze(0))\n","\n","plot_attention(img1, caps, alphas)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#show any 1\n","dataiter = iter(data_loader)\n","images,_ = next(dataiter)\n","\n","img = images[0].detach().clone()\n","img1 = images[0].detach().clone()\n","caps,alphas = get_caps_from(img.unsqueeze(0))\n","\n","plot_attention(img1, caps, alphas)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list(dataset.vocab.itos.values())"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def get_caps_from(features_tensors):\n","    #generate the caption\n","    model.eval()\n","    with torch.no_grad():\n","        features = model.encoder(features_tensors.to(device))\n","        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","        caption = ' '.join(caps)\n","        show_image(features_tensors[0],title=caption)\n","    \n","    return caps,alphas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataiter = iter(data_loader)\n","img, cap = next(dataiter)\n","cap[0]\n","[dataset.vocab.itos[idx] for idx in cap[0].numpy()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset.captions"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","torch.Size([64, 3, 224, 224])\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n","       device='privateuseone:0')\n"]},{"ename":"TypeError","evalue":"sequence item 0: expected str instance, list found","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m realcaptions, predicted\n\u001b[0;32m---> 43\u001b[0m real, pred \u001b[39m=\u001b[39m test(model, data_loader, criterion)\n","Cell \u001b[0;32mIn[38], line 28\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, data, criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder(img)\n\u001b[1;32m     27\u001b[0m caps,alphas \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mgenerate_caption(features,vocab\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m---> 28\u001b[0m caption \u001b[39m=\u001b[39m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(caps)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(caption)\n\u001b[1;32m     30\u001b[0m predicted\u001b[39m.\u001b[39mappend(caption)\n","\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"]}],"source":["BATCH_SIZE=64\n","\n","\n","def test(model, data, criterion):\n","    loss = 0\n","    \n","    realcaptions = []\n","    predicted=[]\n","    model.eval()\n","    i=0\n","    for image, realcaption in data:\n","        i+=1\n","        print(i)\n","        \n","        image = image.to(device)\n","        img = image.detach().clone()\n","               \n","        \n","        vocab = list(dataset.vocab.itos.values())\n","        \n","        realcaption = [[vocab[int(idx)] for idx in array] for array in realcaption.cpu().numpy()]\n","        realcaptions.append(realcaption)\n","        \n","        with torch.no_grad():\n","            print(img.shape)\n","            features = model.encoder(img)\n","            caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","            caption = ' '.join(caps)\n","            print(caption)\n","            predicted.append(caption)\n","       \n","        # compute training reconstruction loss\n","        #loss = criterion(caps.view(-1, vocab_size), alphas.reshape(-1))\n"," \n","        # add the mini-batch training loss to epoch loss\n","        #loss += loss.item()\n","        #print(realcaption)\n","    \n","    # compute the epoch test loss\n","    loss = loss / len(data)\n","    return realcaptions, predicted\n","\n","real, pred = test(model, data_loader, criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["real[0][0]"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["'a'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["dataset.vocab.itos[4]"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
